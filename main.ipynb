{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Project\n",
    "# Wikipedia Search Engine\n",
    "\n",
    "\n",
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from flask import Flask, render_template, request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Flask and the NLTK components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set the search terms and define number of articles\n",
    "search_terms = [\"NFL\", \"NBA\", \"MLB\", \"NHL\"]\n",
    "article_number = 25\n",
    "\n",
    "# Declare the search word(s) that comes after the search terms\n",
    "search_suffix = \"information\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to retrieve HTML content from a given URL using the requests library. It then uses BeautifulSoup to parse the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return soup\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The get_urls() function is responsible for extracting a list of Wikipedia article URLs related to a specific search term from the search results page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(url):   \n",
    "    global article_number     \n",
    "    soup = get_data(url)\n",
    "\n",
    "    # Find all relevant links in the page\n",
    "    results = soup.findAll(\"div\", {\"class\": \"mw-search-result-heading\"})  \n",
    "    urls = [] \n",
    "\n",
    "    for result in results:\n",
    "            link = result.find(\"a\")\n",
    "\n",
    "            # Add the necessary https text\n",
    "            link = f\"https://en.wikipedia.org{link.get('href')}\"  \n",
    "            urls.append(link)\n",
    "    \n",
    "    # Each page displays 20 links so we go to next page and add more links\n",
    "    next_page = soup.find(\"div\", {\"class\": \"mw-pager-navigation-bar\"})\n",
    "    next_page_url = next_page.find(\"a\")\n",
    "    next_page_url = f\"https://en.wikipedia.org{next_page_url.get('href')}\" \n",
    "    \n",
    "    soup = get_data(next_page_url)\n",
    "\n",
    "    # Find all relevant links in the page\n",
    "    results = soup.findAll(\"div\", {\"class\": \"mw-search-result-heading\"}) \n",
    "\n",
    "    # Add more links. Limit the extra links to 20.\n",
    "    if article_number > 40:\n",
    "        article_number = 40\n",
    "\n",
    "    for i in range (article_number - 20):\n",
    "            link = results[i].find(\"a\")\n",
    "\n",
    "            # Add the necessary https text\n",
    "            link = f\"https://en.wikipedia.org{link.get('href')}\"  \n",
    "            urls.append(link)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is a web scraper tailored for scraping information from Wikipedia articles related to American sports, including metadata, text content, categories, links, references, and revision history. The scraped data is structured and stored in a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sports_articles():\n",
    "    global search_bm25\n",
    "\n",
    "    # Generate dataset for the search terms\n",
    "    articles = []\n",
    "    \n",
    "    for term in search_terms:        \n",
    "        url = f\"https://en.wikipedia.org/w/index.php?search={term} {search_suffix}\"\n",
    "\n",
    "        # Get a list of 25 urls relevant to the search term\n",
    "        urls = get_urls(url)\n",
    "\n",
    "        for url in urls: \n",
    "            soup = get_data(url)\n",
    "\n",
    "            # Find the Author, Title, and Article ID\n",
    "            cite_page = soup.find('li', {'id': 't-cite'}).find('a')\n",
    "\n",
    "            if cite_page:\n",
    "                link = f\"https://en.wikipedia.org{cite_page.get('href')}\"\n",
    "                cite_data = get_data(link) \n",
    "\n",
    "                if cite_data:              \n",
    "                    cite_data = cite_data.find('div', {'class': 'plainlinks'}).findAll('li')\n",
    "\n",
    "                    # Get the relevent data\n",
    "                    for data in cite_data:\n",
    "                        data = data.get_text()\n",
    "\n",
    "                        # Get the title\n",
    "                        if 'Page name' in data:\n",
    "                            title = data.split(': ')[1]\n",
    "                        \n",
    "                        # Get the title\n",
    "                        if 'Author' in data:\n",
    "                            author_editor = data.split(': ')[1]\n",
    "                        \n",
    "                        # Get the retrival date\n",
    "                        if 'Date retrieved' in data:\n",
    "                            date_retrived = data.split(': ')[1]\n",
    "                        \n",
    "                        # Get the creation/last modification date\n",
    "                        if 'Date of last revision' in data:\n",
    "                            modification_date = data.split(': ')[1]\n",
    "\n",
    "                        # Get the title\n",
    "                        if 'Page Version ID' in data:\n",
    "                            article_id = data.split(': ')[1]          \n",
    "            \n",
    "            # Get the text content\n",
    "            text_list = []\n",
    "            text_soup = soup.find('div', {'class': 'mw-content-ltr mw-parser-output'})\n",
    "            texts = text_soup.findAll('p')\n",
    "\n",
    "            for text in texts:\n",
    "                text_list.append(text.get_text())\n",
    "\n",
    "            text = \" \".join(text_list)\n",
    "\n",
    "            # Get the word count\n",
    "            words = text.split()\n",
    "            word_count = len(words)\n",
    "            \n",
    "            # Get the categories from the vector-toc-contents\n",
    "            vectors = soup.find('ul', {'class': 'vector-toc-contents'})\n",
    "            vectors = vectors.findAll('a', {'class': 'vector-toc-link'})\n",
    "            \n",
    "            # Get the categories while ignoring the first blank output\n",
    "            categories = []\n",
    "\n",
    "            for vector in vectors[1:]:\n",
    "                category = vector.get('href').replace('#', '')\n",
    "                categories.append(category)\n",
    "\n",
    "            # Get the links in the page\n",
    "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "            # Get the references in the page\n",
    "            references = [a['href'] for a in soup.find_all('a', {'class': 'external text'}, href=True)]\n",
    "           \n",
    "            # Revision History\n",
    "            revision_history = soup.find('li', {'id': 'ca-history'}).find('a')\n",
    "            link = f\"https://en.wikipedia.org{revision_history.get('href')}\" \n",
    "\n",
    "            soup_history = get_data(link)\n",
    "\n",
    "            # Find all revision dates in the revision history table\n",
    "            revision_data = soup_history.select(\".mw-changeslist-date\")\n",
    "\n",
    "            revision_dates = []\n",
    "\n",
    "            # Extract and print the revision dates\n",
    "            for date in revision_data:\n",
    "                revision_dates.append(date.get_text())            \n",
    "                        \n",
    "            articles.append({\"url\": url, \"title\": title, \"text\": text, \"categories\": categories, \"links\":links,\n",
    "                             \"references\": references, \"last_modification_date\": modification_date,\n",
    "                             \"revision_history\": revision_dates, \"author_editor\": author_editor,\n",
    "                             \"article_id\": article_id, \"date_retrieved\": date_retrived, \"word_count\": word_count})\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text: tokenize, remove non alphanumerical characters, remove stopwords, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # split the text into individual words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove punctuation from each word\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "\n",
    "    filtered_tokens = []\n",
    "\n",
    "    # Filter out tokens that are stopwords like \"the\" \"and\" \"an\"\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            # Reduces words to their base form\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            filtered_tokens.append(stemmed_token)\n",
    "\n",
    "    # Return the final preprocessed text\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build_bm25_index(articles) function is responsible for constructing a BM25 index based on a collection of articles. The function takes a list of articles as its input parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bm25_index(articles):\n",
    "    # Preprocess text and tokenize for each article\n",
    "    tokenized_corpus = []\n",
    "\n",
    "    for article in articles:\n",
    "        # Process the text in the articles\n",
    "        tokenized_text = preprocess_text(article[\"text\"]).split(\" \")\n",
    "        tokenized_corpus.append(tokenized_text)\n",
    "        \n",
    "    # Create BM25 index\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next function takes a user query, utilizes the BM25 index to rank articles, identifies the relevant articles, and returns them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bm25(query, bm25, articles):  \n",
    "    # Process the user query\n",
    "    tokenized_query = preprocess_text(query).split(\" \")\n",
    "\n",
    "    # Get the ranking scores\n",
    "    scores = sorted(bm25.get_scores(tokenized_query), reverse=True)\n",
    "\n",
    "    # Get the count of scores greater that 40% of the highest score\n",
    "    threshold = 0.4  # 40%\n",
    "\n",
    "    highest_score = scores[0]\n",
    "    n = sum(1 for score in scores if score > threshold * highest_score)\n",
    "\n",
    "    # Get the relevant articles from the search\n",
    "    relevant_articles = bm25.get_top_n(tokenized_query, articles, n=n)\n",
    "\n",
    "    return relevant_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a webpage that handles both GET and POST requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():    \n",
    "    global sports_articles\n",
    "    global bm25 \n",
    "\n",
    "    if request.method == 'POST':\n",
    "        user_query = request.form['query']\n",
    "        relevant_articles = search_bm25(user_query, bm25, sports_articles)\n",
    "\n",
    "        results = []\n",
    "        for i in range(len(relevant_articles)):\n",
    "            result = {\"title\": relevant_articles[i]['title'], \"url\": relevant_articles[i]['url']}\n",
    "            results.append(result)\n",
    "        \n",
    "        return render_template('index.html', query=user_query, results=results)\n",
    "    else:\n",
    "        return render_template('index.html', message=\"No relevant articles found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [27/Nov/2023 21:53:16] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:53:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:53:18] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:53:39] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:55:19] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:55:47] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:56:03] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:56:15] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:56:52] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:57:10] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:57:25] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:57:46] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Nov/2023 21:58:02] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Get the articles and index them using bm25\n",
    "    sports_articles = scrape_sports_articles()\n",
    "    bm25 = build_bm25_index(sports_articles)\n",
    "\n",
    "    # Run the Flask app\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
